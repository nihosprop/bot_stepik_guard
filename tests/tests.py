import json
import logging
import re
from pathlib import Path

import pymorphy3 as pymorphy2
from Levenshtein import distance
from better_profanity import profanity

from filters.toxicity_classifiers import RussianToxicityClassifier
from filters.patterns import DataProfanity
from tests_cases import TestCases

logger_tests = logging.getLogger(__name__)

BAD_WORDS_PATH = Path(__file__).parent.parent / "badwords.json"
TECHNICAL_WORDS_PATH = (
    Path(__file__).parent.parent / 'filters' / 'technical_words.json')


class TestProfanityFilter:
    
    def __init__(self,
                 bad_words_file=BAD_WORDS_PATH,
                 technical_words_file=TECHNICAL_WORDS_PATH):
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è better_profanity
        profanity.load_censor_words()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pymorphy2
        self.morph = pymorphy2.MorphAnalyzer()
        
        # —Å–ª–æ–≤–∞—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π
        self.data_mapping = DataProfanity.CHAR_REPLACEMENT_MAP
        self.min_word_length = 4
        self.special_chars = set('0123456789!@#$%^&*')
        profanity.CHARS_MAPPING.update(self.data_mapping)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
        self.bad_words = []
        
        if bad_words_file:
            try:
                with open(bad_words_file, 'r', encoding='utf-8') as json_f:
                    self.bad_words = json.load(json_f)
                    logger_tests.debug(f'–î–æ–±–∞–≤–ª—è—é—Ç—Å—è bad_words')
                    profanity.add_censor_words(self.bad_words)
            
            except (FileNotFoundError, json.JSONDecodeError) as err:
                logger_tests.error(
                    f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {bad_words_file}:{err}")
                self.bad_words = []
            except Exception as err:
                logger_tests.error(f'–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON: {err}', exc_info=True)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤(—Å–ª–æ–≤)
        self.tech_keywords = []
        try:
            with open(technical_words_file, 'r', encoding='utf-8') as json_f:
                self.tech_words = json.load(json_f)
                logger_tests.debug(f'–î–æ–±–∞–≤–ª—è—é—Ç—Å—è —Ç–µ—Ö —Å–ª–æ–≤–∞')
        except Exception as err:
            logger_tests.error(f'–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON: {err}', exc_info=True)
        
        # 4. –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
        self.base_pattern = re.compile(
            DataProfanity.base_pattern, flags=re.IGNORECASE)
        
        self.additional_patterns = [re.compile(pattern, flags=re.IGNORECASE) for
            pattern in DataProfanity.additional_patterns]
        
        # 5. –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞
        self.word_pattern = re.compile(r'\b\w+\b')
    
    def is_profanity(self, text: str) -> bool:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        :param text:
        :return bool:
        """
        
        if self._is_technical_text(text):
            print(f'–ü—Ä–æ–ø—É—â–µ–Ω–æ (—Ç–µ—Ö. —Ç–µ–∫—Å—Ç): {text}')
            return False
        
        if any(
            symbol in text for symbol in
                {'=', '(', ')', 'print', 'def', 'class'}):
            print(f'–ü—Ä–æ–ø—É—â–µ–Ω–æ (–∫–æ–¥/—Å–∫–æ–±–∫–∏): {text}')
            return False
        
        if len(set(text)) == 1:
            print(f'–ü—Ä–æ–ø—É—â–µ–Ω–æ (–ø–æ–≤—Ç–æ—Ä —Å–∏–º–≤–æ–ª–æ–≤): {set(text)=}')
            return False
        
        if text.isdigit():
            print(f'–ü—Ä–æ–ø—É—â–µ–Ω–æ (—Ü–∏—Ñ—Ä—ã):{text}')
            return False
        
        simple_text = text.lower().split()
        for word in simple_text:
            if word in self.bad_words:
                print(f'–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ simple_text bad_words: {word}')
                return True
        
        text = text.replace(" ", "")
        normalized_text = self._normalize_text(text)
        text_lower = str(normalized_text).lower()
        
        if len(text_lower.strip()) < 3:
            print(f'–ü—Ä–æ–ø—É—â–µ–Ω–æ: –¥–ª–∏–Ω–∞ –º–µ–Ω—å—à–µ 3—Ö: {text_lower}')
            return False
        
        # 1. –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ better_profanity
        if profanity.contains_profanity(text_lower):
            # logger_tests.warning(
            #     '–§–∏–ª—å—Ç—Ä 1 better_profanity(–ø–æ–ª–Ω–æ–µ '
            #     '—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)')
            print(f'–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ better_profanity: {text}')
            return True
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º
        if self.base_pattern.search(text_lower):
            print(f'–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ base_pattern: {text}')
            return True
        
        for pattern in self.additional_patterns:
            if pattern.search(text.lower()):
                print(f'–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ additional_patterns: {text.lower()}')
                return True
        
        for pattern in self.additional_patterns:
            if pattern.search(text_lower):
                print(f'–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ additional_patterns: {text_lower}')
                return True
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–ø–∏—Å–∫—É —Å–ª–æ–≤ (—Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫)
        words = re.findall(r'\w+', text_lower)
        if any(word in self.bad_words for word in words):
            print(
                f'–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–ø–∏—Å–∫—É —Å–ª–æ–≤ (—Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫): {words}')
            return True
        
        # 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if self._check_levenshtein(text):
            print(f'–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ: –§–∏–ª—å—Ç—Ä 5 "Levenshtein": {text_lower}')
            return True
        print(f'–¢–µ–∫—Å—Ç –ø—Ä–æ—à–µ–ª –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã: {text}')
        return False
    
    def _is_technical_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –º–∞—Ç –≤ —Ç–∞–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)"""
        words = re.findall(r'\w+', text.lower())
        for word in words:
            parsed = self.morph.parse(word)[0]  # –±–µ—Ä–µ–º —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ä–∞–∑–±–æ—Ä
            normal_form = parsed.normal_form  # –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞
            if normal_form in self.tech_keywords:  # –µ—Å–ª–∏ —ç—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω
                return True
        return False
    
    def _is_technical_word(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º —Ç–µ—Ä–º–∏–Ω–æ–º (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –µ–≥–æ –≤ –ø—Ä–æ–≤–µ—Ä–∫–∞—Ö)."""
        parsed = self.morph.parse(word.lower())[0]
        normal_form = parsed.normal_form  # –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞
        return normal_form in self.tech_keywords
    
    def _normalize_text(self, text: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–º–µ–Ω—è–µ–º –≤—Å–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –∏ –ø–æ—Ö–æ–∂–∏–µ –±—É–∫–≤—ã
        normalized = []
        for char in text.lower():
            replacement = char
            for base_char, variants in self.data_mapping.items():
                if char in variants:
                    replacement = base_char
                    break
            normalized.append(replacement)
        normalized_text = ''.join(normalized)
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä "–ø—Ä—Ä–∏–≤–µ—Ç" -> "–ø—Ä–∏–≤–µ—Ç")
        normalized_text = re.sub(r'(.)\1+', r'\1', normalized_text)
        return normalized_text
    
    def _is_valid_match(self, candidate: str, bad_word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–∞–ª–∏–¥–Ω—ã–º"""
        # –ï—Å–ª–∏ –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–µ –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã/—Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã - —Å—á–∏—Ç–∞–µ–º –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–º
        if any(c in self.special_chars for c in candidate):
            return True
        
        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ (3-4 —Å–∏–º–≤–æ–ª–∞) —Ç—Ä–µ–±—É–µ–º —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        if len(bad_word) <= 3:
            return candidate == bad_word
        
        # –î–ª—è —Å–ª–æ–≤ –∏–∑ 5 —Å–∏–º–≤–æ–ª–æ–≤ - –º–∞–∫—Å–∏–º—É–º 1 –æ—à–∏–±–∫–∞
        if len(bad_word) == 5:
            return distance(candidate, bad_word) <= 1
        
        # –î–ª—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤ - –º–∞–∫—Å–∏–º—É–º 2 –æ—à–∏–±–∫–∏
        return distance(candidate, bad_word) <= 2
    
    def _check_levenshtein(self, phrase: str) -> bool:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        normalized = self._normalize_text(phrase)
        print(f'Normalized: {normalized}: Not normalized: {phrase}')
        words = re.findall(r'\b\w+\b', normalized)  # –≤—ã–¥–µ–ª—è–µ–º —Ü–µ–ª—ã–µ —Å–ª–æ–≤–∞
        
        for bad_word in self.bad_words:
            bw_len = len(bad_word)
            
            # –ù–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
            if bw_len < self.min_word_length:
                continue
            
            for candidate in words:
                c_len = len(candidate)
                
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –∫–æ—Ä–æ—á–µ min_word_length
                if c_len < self.min_word_length:
                    continue
                
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –¥–ª–∏–Ω–µ
                if abs(c_len - bw_len) > 2:  # –¥–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–Ω–∏—Ü—É –¥–æ 1 —Å–∏–º–≤–æ–ª–æ–≤
                    continue
                
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                if candidate == bad_word:
                    logger_tests.warning(f'–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {bad_word}')
                    return True
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ (—É–∂–µ—Å—Ç–æ—á—ë–Ω–Ω–∞—è)
                max_allowed_distance = 1 if bw_len <= 6 else 2
                
                # –ï—Å–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç ‚Äî —á–∞—Å—Ç—å –¥—Ä—É–≥–æ–≥–æ —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–∫–æ–¥" –≤ "–∫–æ–¥–µ–∫—Å"), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if candidate in bad_word or bad_word in candidate:
                    continue
                    
                # –ï—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
                if distance(candidate, bad_word) <= max_allowed_distance:
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–ª–æ–≤–æ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —á–∞—Å—Ç—å—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
                    if not self._is_technical_word(candidate):
                        logger_tests.warning(
                            f'–ù–∞–π–¥–µ–Ω–æ –ø–æ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω—É: {bad_word} '
                            f'(–∫–∞–Ω–¥–∏–¥–∞—Ç: {candidate}, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance(candidate, bad_word)})')
                        return True
        
        return False


def test_comment_filter():
    profanity_filter = TestProfanityFilter()
    passed = 0
    
    for comment, expected in TestCases.test_cases:
        result = profanity_filter.is_profanity(text=comment)
        if result == expected:
            # print(f'–¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω: {comment}')
            passed += 1
        
        else:
            print(
                f"üü¢–¢–µ—Å—Ç –ø—Ä–æ–≤–∞–ª–µ–Ω: '{comment}' | –û–∂–∏–¥–∞–ª–æ—Å—å: {expected}, –ü–æ–ª—É—á–µ–Ω–æ: {result}")
    
    print(
        f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç: {passed} –∏–∑ {len(TestCases.test_cases)} —Ç–µ—Å—Ç–æ–≤ "
        f"–ø—Ä–æ–π–¥–µ–Ω–æ")
    print(
        f"–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞:"
        f" {passed / len(TestCases.test_cases) * 100:.2f}%")


if __name__ == "__main__":
    test_comment_filter()
