import logging
import json
import re
from pathlib import Path
import pymorphy3 as pymorphy2
from Levenshtein import distance
from better_profanity import profanity
from aiogram.filters import BaseFilter
from aiogram.types import CallbackQuery, Message

from filters.patterns import DataProfanity

logger_filters = logging.getLogger(__name__)


class AccessRightsFilter(BaseFilter):
    def __init__(self, flag_admins: bool = False):
        self.flag_admins = flag_admins
    
    async def __call__(self,
                       msg: Message | CallbackQuery,
                       owners: list[int]) -> bool:
        user_tg_id = msg.from_user.id
        
        return user_tg_id in owners


class ProfanityFilter:
    
    BAD_WORDS_PATH = Path(__file__).parent.parent / "badwords.json"
    TECHNICAL_WORDS_PATH = (
        Path(__file__).parent.parent / 'filters' / 'technical_words.json')
    
    def __init__(self,
                 bad_words_file=BAD_WORDS_PATH,
                 technical_words_file=TECHNICAL_WORDS_PATH):
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è better_profanity
        profanity.load_censor_words()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pymorphy2
        self.morph = pymorphy2.MorphAnalyzer()
        
        # —Å–ª–æ–≤–∞—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π
        self.data_mapping = DataProfanity.CHAR_REPLACEMENT_MAP
        self.min_word_length = 3
        self.special_chars = set('0123456789!@#$%^&*')
        profanity.CHARS_MAPPING.update(self.data_mapping)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
        self.bad_words = []
        
        if bad_words_file:
            try:
                with open(bad_words_file, 'r', encoding='utf-8') as json_f:
                    self.bad_words = json.load(json_f)
                    logger_filters.debug(f'–î–æ–±–∞–≤–ª—è—é—Ç—Å—è bad_words')
                    profanity.add_censor_words(self.bad_words)
            
            except (FileNotFoundError, json.JSONDecodeError) as err:
                logger_filters.error(
                    f"üü¢–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {bad_words_file}:{err}")
                self.bad_words = []
            except Exception as err:
                logger_filters.error(f'üü¢–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON: {err}', exc_info=True)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤(—Å–ª–æ–≤)
        self.tech_keywords = []
        try:
            with open(technical_words_file, 'r', encoding='utf-8') as json_f:
                self.tech_words = json.load(json_f)
                logger_filters.debug(f'–î–æ–±–∞–≤–ª—è—é—Ç—Å—è —Ç–µ—Ö —Å–ª–æ–≤–∞')
        except Exception as err:
            logger_filters.error(f'üü¢–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON: {err}', exc_info=True)
        
        # 4. –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
        self.base_pattern = re.compile(
            DataProfanity.base_pattern, flags=re.IGNORECASE)
        
        self.additional_patterns = [re.compile(pattern, flags=re.IGNORECASE) for
            pattern in DataProfanity.additional_patterns]
        
        # 5. –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞
        self.word_pattern = re.compile(r'\b\w+\b')
    
    async def is_profanity(self, text: str) -> bool:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        :param text:
        :return bool:
        """
        
        if await self._is_technical_text(text):
            logger_filters.debug('–ü—Ä–æ–ø—É—â–µ–Ω–æ (—Ç–µ—Ö. —Ç–µ–∫—Å—Ç)')
            return False
        
        if any(
            symbol in text for symbol in
                {'=', '(', ')', 'print', 'def', 'class'}):
            logger_filters.debug('–ü—Ä–æ–ø—É—â–µ–Ω–æ (–∫–æ–¥/—Å–∫–æ–±–∫–∏)')
            return False
        
        if len(set(text)) == 1:
            logger_filters.debug(f'–ü—Ä–æ–ø—É—â–µ–Ω–æ (–ø–æ–≤—Ç–æ—Ä —Å–∏–º–≤–æ–ª–æ–≤): {set(text)=}')
            return False
        
        if text.isdigit():
            logger_filters.debug(f'–ü—Ä–æ–ø—É—â–µ–Ω–æ (—Ü–∏—Ñ—Ä—ã):{text}')
            return False
        
        simple_text = text.lower().split()
        for word in simple_text:
            if word in self.bad_words:
                logger_filters.debug(f'üü¢–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ simple_text bad_words: {word}')
                return True
        
        text = text.replace(" ", "")
        normalized_text = await self._normalize_text(text)
        text_lower = str(normalized_text).lower()
        
        if len(text_lower.strip()) < 3:
            logger_filters.debug(f'–ü—Ä–æ–ø—É—â–µ–Ω–æ: –¥–ª–∏–Ω–∞ –º–µ–Ω—å—à–µ 3—Ö: {text_lower}')
            return False
        
        # 1. –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ better_profanity
        if profanity.contains_profanity(text_lower):
            # logger_tests.warning(
            #     '–§–∏–ª—å—Ç—Ä 1 better_profanity(–ø–æ–ª–Ω–æ–µ '
            #     '—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)')
            logger_filters.debug(f'üü¢–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ better_profanity: {text}')
            return True
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º
        if self.base_pattern.search(text_lower):
            logger_filters.debug(f'üü¢–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ base_pattern: {text}')
            return True
        
        for pattern in self.additional_patterns:
            if pattern.search(text.lower()):
                logger_filters.debug(f'üü¢–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ additional_p'
                                f'atterns: {text.lower()}')
                return True
        
        for pattern in self.additional_patterns:
            if pattern.search(text_lower):
                logger_filters.debug(f'üü¢–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ additional_patterns: {
                text_lower}')
                return True
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–ø–∏—Å–∫—É —Å–ª–æ–≤ (—Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫)
        words = re.findall(r'\w+', text_lower)
        if any(word in self.bad_words for word in words):
            logger_filters.debug(
                f'üü¢–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–ø–∏—Å–∫—É —Å–ª–æ–≤ (—Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫): {words}')
            return True
        
        # 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if self._check_levenshtein(text_lower):
            logger_filters.debug('üü¢–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ: –§–∏–ª—å—Ç—Ä 5 "Levenshtein"')
            return True
        logger_filters.debug('–¢–µ–∫—Å—Ç –ø—Ä–æ—à–µ–ª –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã')
        return False
    
    async def _is_technical_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –º–∞—Ç –≤ —Ç–∞–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)"""
        words = re.findall(r'\w+', text.lower())
        for word in words:
            parsed = self.morph.parse(word)[0]  # –±–µ—Ä–µ–º —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ä–∞–∑–±–æ—Ä
            normal_form = parsed.normal_form  # –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞
            if normal_form in self.tech_keywords:  # –µ—Å–ª–∏ —ç—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω
                return True
        return False
    
    async def _is_technical_word(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º —Ç–µ—Ä–º–∏–Ω–æ–º (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –µ–≥–æ –≤ –ø—Ä–æ–≤–µ—Ä–∫–∞—Ö)."""
        parsed = self.morph.parse(word.lower())[0]
        normal_form = parsed.normal_form  # –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞
        return normal_form in self.tech_keywords
    
    async def _normalize_text(self, text: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–º–µ–Ω—è–µ–º –≤—Å–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –∏ –ø–æ—Ö–æ–∂–∏–µ –±—É–∫–≤—ã
        normalized = []
        for char in text.lower():
            replacement = char
            for base_char, variants in self.data_mapping.items():
                if char in variants:
                    replacement = base_char
                    break
            normalized.append(replacement)
        normalized_text = ''.join(normalized)
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä "–ø—Ä—Ä–∏–≤–µ—Ç" -> "–ø—Ä–∏–≤–µ—Ç")
        normalized_text = re.sub(r'(.)\1+', r'\1', normalized_text)
        return normalized_text
    
    # TODO –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ _is_valid_match
    async def _is_valid_match(self, candidate: str, bad_word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–∞–ª–∏–¥–Ω—ã–º"""
        # –ï—Å–ª–∏ –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–µ –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã/—Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã - —Å—á–∏—Ç–∞–µ–º –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–º
        if any(c in self.special_chars for c in candidate):
            return True
        
        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ (3-4 —Å–∏–º–≤–æ–ª–∞) —Ç—Ä–µ–±—É–µ–º —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        if len(bad_word) <= 3:
            return candidate == bad_word
        
        # –î–ª—è —Å–ª–æ–≤ –∏–∑ 5 —Å–∏–º–≤–æ–ª–æ–≤ - –º–∞–∫—Å–∏–º—É–º 1 –æ—à–∏–±–∫–∞
        if len(bad_word) == 5:
            return distance(candidate, bad_word) <= 1
        
        # –î–ª—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤ - –º–∞–∫—Å–∏–º—É–º 2 –æ—à–∏–±–∫–∏
        return distance(candidate, bad_word) <= 2
    
    async def _check_levenshtein(self, phrase: str) -> bool:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        normalized = await self._normalize_text(phrase)
        words = re.findall(r'\b\w+\b', normalized)  # –≤—ã–¥–µ–ª—è–µ–º —Ü–µ–ª—ã–µ —Å–ª–æ–≤–∞
        
        for bad_word in self.bad_words:
            bw_len = len(bad_word)
            
            # –ù–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
            if bw_len < self.min_word_length:
                continue
            
            for candidate in words:
                c_len = len(candidate)
                
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –∫–æ—Ä–æ—á–µ min_word_length
                if c_len < self.min_word_length:
                    continue
                
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –¥–ª–∏–Ω–µ
                if abs(c_len - bw_len) > 2:  # –¥–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–Ω–∏—Ü—É –¥–æ 1 —Å–∏–º–≤–æ–ª–æ–≤
                    continue
                
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                if candidate == bad_word:
                    logger_filters.warning(f'üü¢–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {bad_word}')
                    return True
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ (—É–∂–µ—Å—Ç–æ—á—ë–Ω–Ω–∞—è)
                max_allowed_distance = 1 if bw_len <= 6 else 2
                
                # –ï—Å–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç ‚Äî —á–∞—Å—Ç—å –¥—Ä—É–≥–æ–≥–æ —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–∫–æ–¥" –≤ "–∫–æ–¥–µ–∫—Å"), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if candidate in bad_word or bad_word in candidate:
                    continue
                
                # –ï—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
                if distance(candidate, bad_word) <= max_allowed_distance:
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–ª–æ–≤–æ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —á–∞—Å—Ç—å—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
                    if not await self._is_technical_word(candidate):
                        logger_filters.warning(
                            f'üü¢–ù–∞–π–¥–µ–Ω–æ –ø–æ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω—É: {bad_word} '
                            f'(–∫–∞–Ω–¥–∏–¥–∞—Ç: {candidate}, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance(candidate, bad_word)})')
                        return True
        
        return False
